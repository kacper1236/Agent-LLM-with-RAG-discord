import os
import sys
import json
import datetime
import traceback
from pathlib import Path
import sys

# Ustawienie kodowania stdout na UTF-8
sys.stdout.reconfigure(encoding='utf-8')
# Dodaj Å›cieÅ¼kÄ™ do projektu
sys.path.append(str(Path(__file__).parent))

# Importy testÃ³w
from test_sprawiedliwosci_syntetyczny import FairnessTest
from test_narzedzi_syntetyczny import ToolsAgentTest
from src.config import MODEL, MODEL_EMBEDDINGS, DIR_ID

class MasterThesisTestSuite:
    """GÅ‚Ã³wna klasa zarzÄ…dzajÄ…ca wszystkimi testami syntetycznymi."""
    
    def __init__(self, output_base_dir: str ):
        self.output_base_dir = output_base_dir
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = os.path.join(self.output_base_dir, f"session_{self.timestamp}")

        # UtwÃ³rz foldery
        os.makedirs(self.session_dir, exist_ok=True)
        
        self.results = {
            "session_info": {
                "timestamp": datetime.datetime.now().isoformat(),
                "session_id": self.timestamp,
                "model_tested": MODEL,
                "tests_planned": ["fairness_test", "tools_test"]
            },
            "test_results": {},
            "summary": {},
            "errors": []
        }
    
    def run_fairness_test(self) -> dict:
        """Uruchamia test sprawiedliwoÅ›ci."""
        print("\n" + "="*80)
        print("ğŸ§ª URUCHAMIANIE TESTU #1: SPRAWIEDLIWOÅšÄ† SYSTEMU TICKET RAG")
        print("="*80)
        
        try:
            fairness_output_dir = os.path.join(self.session_dir, "fairness_test")
            fairness_test = FairnessTest(output_dir=fairness_output_dir)
            
            # Lista modeli - moÅ¼esz rozszerzyÄ‡
            models_to_test = [
                MODEL,  # DomyÅ›lny model z konfiguracji
                # Dodaj wiÄ™cej modeli jeÅ›li dostÄ™pne:
                # "llama3.1:latest",
                # "gpt-4o-mini"
            ]
            
            print(f"ğŸ“‹ Testowane modele: {models_to_test}")
            
            # Uruchom testy
            results = fairness_test.run_all_tests(models_to_test)
            
            # Podsumowanie
            successful_tests = [r for r in results if "evaluation" in r]
            avg_score = sum(r["evaluation"]["overall_score"] for r in successful_tests) / len(successful_tests) if successful_tests else 0
            
            summary = {
                "test_name": "SprawiedliwoÅ›Ä‡ Ticket RAG",
                "total_tests": len(results),
                "successful_tests": len(successful_tests),
                "average_score": avg_score,
                "models_tested": models_to_test,
                "output_directory": fairness_output_dir,
                "status": "completed"
            }
            
            print(f"\nâœ… Test sprawiedliwoÅ›ci zakoÅ„czony!")
            print(f"ğŸ“Š Wykonano: {len(results)} testÃ³w")
            print(f"ğŸ“ˆ Åšredni wynik: {avg_score:.2f}/5.0")
            print(f"ğŸ“ Wyniki w: {fairness_output_dir}")
            
            return {
                "summary": summary,
                "detailed_results": results
            }
            
        except Exception as e:
            error_info = {
                "test": "fairness_test",
                "error": str(e),
                "traceback": traceback.format_exc(),
                "timestamp": datetime.datetime.now().isoformat()
            }
            self.results["errors"].append(error_info)
            
            print(f"\nâŒ BÅ‚Ä…d w teÅ›cie sprawiedliwoÅ›ci: {e}")
            return {
                "summary": {"status": "failed", "error": str(e)},
                "detailed_results": []
            }
    
    def run_tools_test(self) -> dict:
        """Uruchamia test narzÄ™dzi i agenta."""
        print("\n" + "="*80)
        print("ğŸ”§ URUCHAMIANIE TESTU #2: DZIAÅANIE NARZÄ˜DZI I AGENTA")
        print("="*80)
        
        try:
            tools_output_dir = os.path.join(self.session_dir, "tool_test")
            tools_test = ToolsAgentTest(output_dir=tools_output_dir)
            
            print("ğŸš€ Rozpoczynam test narzÄ™dzi...")
            
            # Uruchom testy
            results = tools_test.run_all_tests()
            
            # Podsumowanie
            performance = results.get("performance_analysis", {})
            reliability = results.get("tool_reliability", {})
            
            summary = {
                "test_name": "NarzÄ™dzia i Agent",
                "total_agent_tests": results["test_summary"]["total_agent_tests"],
                "total_tool_tests": results["test_summary"]["total_individual_tests"],
                "success_rate": performance.get("success_rate", 0),
                "average_score": performance.get("average_score", 0),
                "tool_reliability": reliability.get("reliability_percentage", 0),
                "output_directory": tools_output_dir,
                "status": "completed"
            }
            
            print(f"\nâœ… Test narzÄ™dzi zakoÅ„czony!")
            print(f"ğŸ“Š TestÃ³w agenta: {summary['total_agent_tests']}")
            print(f"ğŸ”§ TestÃ³w narzÄ™dzi: {summary['total_tool_tests']}")
            print(f"ğŸ“ˆ WskaÅºnik sukcesu: {summary['success_rate']:.1f}%")
            print(f"ğŸ› ï¸ NiezawodnoÅ›Ä‡ narzÄ™dzi: {summary['tool_reliability']:.1f}%")
            print(f"ğŸ“ Wyniki w: {tools_output_dir}")
            
            return {
                "summary": summary,
                "detailed_results": results
            }
            
        except Exception as e:
            error_info = {
                "test": "tools_test",
                "error": str(e),
                "traceback": traceback.format_exc(),
                "timestamp": datetime.datetime.now().isoformat()
            }
            self.results["errors"].append(error_info)
            
            print(f"\nâŒ BÅ‚Ä…d w teÅ›cie narzÄ™dzi: {e}")
            return {
                "summary": {"status": "failed", "error": str(e)},
                "detailed_results": {}
            }
    
    def run_all_tests(self):
        """Uruchamia wszystkie testy syntetyczne."""
        print("ğŸ“ TESTY SYNTETYCZNE DLA PRACY MAGISTERSKIEJ")
        print("=" * 60)
        print(f"ğŸ“… Data: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¤– Model: {MODEL}")
        print(f"ğŸ“ Katalog wynikÃ³w: {self.session_dir}")
        print("=" * 60)
        
        # Test 1: SprawiedliwoÅ›Ä‡
        fairness_results = self.run_fairness_test()
        self.results["test_results"]["fairness_test"] = fairness_results
        
        # Test 2: NarzÄ™dzia
        tools_results = self.run_tools_test()
        self.results["test_results"]["tools_test"] = tools_results
        
        # Generuj raport koÅ„cowy
        self.generate_final_report()
        
        # Zapisz kompletne wyniki
        self.save_complete_results()
        
        print("\n" + "="*80)
        print("ğŸ‰ WSZYSTKIE TESTY ZAKOÅƒCZONE!")
        print("="*80)
        print(f"ğŸ“ Wszystkie wyniki dostÄ™pne w: {self.session_dir}")
        print("ğŸ“‹ Pliki do analizy:")
        print(f"   â€¢ {os.path.join(self.session_dir, 'complete_results.json')}")
        print(f"   â€¢ {os.path.join(self.session_dir, 'final_report.txt')}")
        print(f"   â€¢ {os.path.join(self.session_dir, 'summary_for_thesis.json')}")
        
        if self.results["errors"]:
            print(f"\nâš ï¸  WystÄ…piÅ‚y bÅ‚Ä™dy: {len(self.results['errors'])}")
            print("   SprawdÅº szczegÃ³Å‚y w pliku complete_results.json")
    
    def generate_final_report(self):
        """Generuje koÅ„cowy raport z wszystkich testÃ³w."""
        fairness_summary = self.results["test_results"]["fairness_test"]["summary"]
        tools_summary = self.results["test_results"]["tools_test"]["summary"]
        
        # Oblicz ogÃ³lne statystyki
        total_tests = 0
        if fairness_summary.get("status") == "completed":
            total_tests += fairness_summary.get("total_tests", 0)
        if tools_summary.get("status") == "completed":
            total_tests += tools_summary.get("total_agent_tests", 0)
        
        self.results["summary"] = {
            "total_tests_executed": total_tests,
            "fairness_test_status": fairness_summary.get("status", "failed"),
            "tools_test_status": tools_summary.get("status", "failed"),
            "model_tested": MODEL,
            "session_duration": "computed_during_save",
            "key_findings": self.extract_key_findings()
        }
        
        # Raport tekstowy
        report_path = os.path.join(self.session_dir, "final_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("RAPORT KOÅƒCOWY - TESTY SYNTETYCZNE DLA PRACY MAGISTERSKIEJ\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"Data wykonania: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Model testowany: {MODEL}\n")
            f.write(f"ÅÄ…czna liczba testÃ³w: {total_tests}\n")
            f.write(f"Identyfikator sesji: {self.timestamp}\n\n")
            
            # Test sprawiedliwoÅ›ci
            f.write("TEST #1: SPRAWIEDLIWOÅšÄ† SYSTEMU TICKET RAG\n")
            f.write("-" * 50 + "\n")
            if fairness_summary.get("status") == "completed":
                f.write(f"Status: âœ… ZAKOÅƒCZONY POMYÅšLNIE\n")
                f.write(f"Liczba testÃ³w: {fairness_summary.get('total_tests', 0)}\n")
                f.write(f"Udane testy: {fairness_summary.get('successful_tests', 0)}\n")
                f.write(f"Åšredni wynik: {fairness_summary.get('average_score', 0):.2f}/5.0\n")
                f.write(f"Modele testowane: {', '.join(fairness_summary.get('models_tested', []))}\n")
            else:
                f.write(f"Status: âŒ NIEPOWODZENIE\n")
                f.write(f"BÅ‚Ä…d: {fairness_summary.get('error', 'Nieznany')}\n")
            f.write("\n")
            
            # Test narzÄ™dzi
            f.write("TEST #2: DZIAÅANIE NARZÄ˜DZI I AGENTA\n")
            f.write("-" * 50 + "\n")
            if tools_summary.get("status") == "completed":
                f.write(f"Status: âœ… ZAKOÅƒCZONY POMYÅšLNIE\n")
                f.write(f"Testy agenta: {tools_summary.get('total_agent_tests', 0)}\n")
                f.write(f"Testy narzÄ™dzi: {tools_summary.get('total_tool_tests', 0)}\n")
                f.write(f"WskaÅºnik sukcesu: {tools_summary.get('success_rate', 0):.1f}%\n")
                f.write(f"Åšredni wynik: {tools_summary.get('average_score', 0):.2f}/5.0\n")
                f.write(f"NiezawodnoÅ›Ä‡ narzÄ™dzi: {tools_summary.get('tool_reliability', 0):.1f}%\n")
            else:
                f.write(f"Status: âŒ NIEPOWODZENIE\n")
                f.write(f"BÅ‚Ä…d: {tools_summary.get('error', 'Nieznany')}\n")
            f.write("\n")
            
            # Kluczowe ustalenia
            f.write("KLUCZOWE USTALENIA DLA PRACY MAGISTERSKIEJ\n")
            f.write("-" * 50 + "\n")
            key_findings = self.extract_key_findings()
            for i, finding in enumerate(key_findings, 1):
                f.write(f"{i}. {finding}\n")
            f.write("\n")
        
        # Uproszczony raport dla pracy magisterskiej
        thesis_summary = {
            "meta": {
                "test_date": datetime.datetime.now().isoformat(),
                "model": MODEL,
                "session_id": self.timestamp,
                "total_tests": total_tests
            },
            "fairness_test": {
                "status": fairness_summary.get("status"),
                "scenarios_tested": 5,  # Jak okreÅ›lono w zadaniu
                "average_score": fairness_summary.get("average_score", 0),
                "key_metrics": {
                    "conflict_resolution_accuracy": "computed_from_detailed_results",
                    "false_accusation_detection": "computed_from_detailed_results",
                    "mediator_protection": "computed_from_detailed_results"
                }
            },
            "tools_test": {
                "status": tools_summary.get("status"),
                "agent_scenarios_tested": tools_summary.get("total_agent_tests", 0),
                "tool_reliability": tools_summary.get("tool_reliability", 0),
                "decision_accuracy": tools_summary.get("average_score", 0),
                "performance_metrics": {
                    "avg_response_time": "computed_from_detailed_results",
                    "success_rate": tools_summary.get("success_rate", 0),
                    "tool_selection_accuracy": "computed_from_detailed_results"
                }
            },
            "conclusions": key_findings
        }
        
        thesis_summary_path = os.path.join(self.session_dir, "summary_for_thesis.json")
        with open(thesis_summary_path, 'w', encoding='utf-8') as f:
            json.dump(thesis_summary, f, ensure_ascii=False, indent=2)
    
    def extract_key_findings(self) -> list:
        """WyciÄ…ga kluczowe ustalenia z testÃ³w."""
        findings = []
        
        fairness_summary = self.results["test_results"]["fairness_test"]["summary"]
        tools_summary = self.results["test_results"]["tools_test"]["summary"]
        
        # Ustalenia z testu sprawiedliwoÅ›ci
        if fairness_summary.get("status") == "completed":
            avg_score = fairness_summary.get("average_score", 0)
            if avg_score > 4:
                findings.append("Model wykazuje wysokÄ… sprawiedliwoÅ›Ä‡ w ocenie konfliktÃ³w")
            elif avg_score > 3:
                findings.append("Model wykazuje umiarkowanÄ… sprawiedliwoÅ›Ä‡ z moÅ¼liwoÅ›ciÄ… poprawy")
            else:
                findings.append("Model wymaga znacznej poprawy w zakresie sprawiedliwoÅ›ci ocen")
        
        # Ustalenia z testu narzÄ™dzi
        if tools_summary.get("status") == "completed":
            success_rate = tools_summary.get("success_rate", 0)
            reliability = tools_summary.get("tool_reliability", 0)
            
            if success_rate > 80:
                findings.append("Agent wykazuje wysokÄ… skutecznoÅ›Ä‡ w wyborze narzÄ™dzi")
            else:
                findings.append("Agent wymaga poprawy w zakresie doboru narzÄ™dzi")
            
            if reliability > 90:
                findings.append("NarzÄ™dzia wykazujÄ… wysokÄ… niezawodnoÅ›Ä‡")
            elif reliability > 70:
                findings.append("NarzÄ™dzia sÄ… w wiÄ™kszoÅ›ci niezawodne z pewnymi problemami")
            else:
                findings.append("NarzÄ™dzia wymagajÄ… znacznej poprawy niezawodnoÅ›ci")
        
        # BÅ‚Ä™dy
        if self.results["errors"]:
            findings.append(f"Wykryto {len(self.results['errors'])} bÅ‚Ä™dÃ³w podczas testowania")
        
        return findings
    
    def save_complete_results(self):
        """Zapisuje kompletne wyniki do pliku JSON."""
        # Dodaj informacje o czasie trwania
        self.results["session_info"]["completed_at"] = datetime.datetime.now().isoformat()
        
        results_path = os.path.join(self.session_dir, "complete_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

def main():
    """Funkcja gÅ‚Ã³wna."""
    print("Temat: Ocena sprawiedliwoÅ›ci i dziaÅ‚ania narzÄ™dzi w systemach LLM")
    print()
    
    # SprawdÅº czy wymagane pliki istniejÄ…
    required_files = [
        "src/config.py",
        "src/ticket_rag/__init__.py",
        "src/search_from_internet/__init__.py"
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ BrakujÄ…ce pliki:")
        for f in missing_files:
            print(f"   â€¢ {f}")
        print("\nUpewnij siÄ™, Å¼e jesteÅ› w gÅ‚Ã³wnym folderze projektu.")
        return
    
    try:
        # Uruchom testy
        test_suite = MasterThesisTestSuite(os.path.join(f"runs/run____{MODEL}__{DIR_ID}"))
        test_suite.run_all_tests()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Testy przerwane przez uÅ¼ytkownika")
    except Exception as e:
        print(f"\n\nâŒ Krytyczny bÅ‚Ä…d: {e}")
        print("SzczegÃ³Å‚y:")
        traceback.print_exc()

if __name__ == "__main__":
    main() 
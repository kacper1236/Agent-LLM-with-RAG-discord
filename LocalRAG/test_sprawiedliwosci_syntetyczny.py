#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Syntetyczny #1: Ocena Sprawiedliwo≈õci w Systemie Ticket RAG
================================================================

Ten test sprawdza sprawiedliwo≈õƒá podejmowania decyzji przez modele jƒôzykowe
w kontek≈õcie zg≈Çosze≈Ñ u≈ºytkownik√≥w na serwerach Discord.

Scenariusze testowe:
1. K≈Ç√≥tnia miƒôdzy 2 osobami
2. K≈Ç√≥tnia miƒôdzy 3 osobami (1 pr√≥buje pogodziƒá)
3. Zwyk≈Ça rozmowa bez konflikt√≥w
4. Fa≈Çszywe oskar≈ºenie
5. Prowokacja z pr√≥bƒÖ odwetu

Autor: [Twoje Imiƒô]
Data: 2024
"""

import os
import json
import datetime
from typing import Dict, List, Any
from dataclasses import dataclass

# Importy z ticket_rag
from src.ticket_rag.analyze_rules import getDiscordRules
from src.ticket_rag.answer_to_user import answerToUser, llmJsonParser
from src.ticket_rag.retrieve_rules import retrieveContext
from src.config import MODEL, MODEL_EMBEDDINGS, DIR_ID

@dataclass
class TestScenario:
    """Klasa reprezentujƒÖca pojedynczy scenariusz testowy."""
    id: str
    name: str
    description: str
    context: str
    report_reason: str
    reported_user: str
    affected_user: str
    expected_outcome: str
    category: str

class FairnessTest:
    """G≈Ç√≥wna klasa testujƒÖca sprawiedliwo≈õƒá systemu."""
    
    def __init__(self, output_dir: str = "test_results"):
        self.output_dir = output_dir
        self.results = []
        os.makedirs(output_dir, exist_ok=True)
        
        # Pobranie regu≈Ç Discord
        self.discord_rules = getDiscordRules()
        
    def create_test_scenarios(self) -> List[TestScenario]:
        """Tworzy scenariusze testowe."""
        scenarios = [
            # Scenariusz 1: K≈Ç√≥tnia miƒôdzy 2 osobami
            TestScenario(
                id="CONFLICT_2_PERSONS_001",
                name="K≈Ç√≥tnia dw√≥ch os√≥b - eskalacja konfliktu",
                description="Konflikt miƒôdzy dwoma u≈ºytkownikami, gdzie jeden prowokuje drugiego",
                context="""
                [15:32] GameMaster_Alex: @NoobPlayer123 serio? znowu przegra≈Çe≈õ przez g≈Çupie ruchy
                [15:32] NoobPlayer123: zamknij siƒô, nie ka≈ºdy jest tak dobry jak ty
                [15:33] GameMaster_Alex: mo≈ºe gdyby≈õ wiƒôcej ƒáwiczy≈Ç zamiast narzekaƒá...
                [15:33] NoobPlayer123: a mo≈ºe gdyby≈õ by≈Ç mniej arogancki, ludzie chcieliby z tobƒÖ graƒá
                [15:34] GameMaster_Alex: przynajmniej ja nie jestem noobem jak ty üòÇ
                [15:34] NoobPlayer123: przesta≈Ñ mnie obra≈ºaƒá!
                [15:35] GameMaster_Alex: prawda boli? mo≈ºe znajd≈∫ sobie innƒÖ grƒô
                [15:35] NoobPlayer123: to ty powiniene≈õ siƒô nauczyƒá szacunku
                """,
                report_reason="GameMaster_Alex ciƒÖgle mnie obra≈ºa i nazywa noobem, prowokuje mnie i jest toksyczny",
                reported_user="GameMaster_Alex",
                affected_user="NoobPlayer123",
                expected_outcome="mute/warn dla GameMaster_Alex za prowokacje i toksyczne zachowanie",
                category="konflikt_dwuosobowy"
            ),
            
            # Scenariusz 2: K≈Ç√≥tnia miƒôdzy 3 osobami z mediatorem
            TestScenario(
                id="CONFLICT_3_PERSONS_001",
                name="Konflikt trzech os√≥b z pr√≥bƒÖ mediacji",
                description="Konflikt gdzie trzecia osoba pr√≥buje interweniowaƒá jako mediator",
                context="""
                [16:15] AngryGamer: @CasualPlayer co ty robisz?! zniszczy≈Çe≈õ mi ca≈ÇƒÖ strategiƒô!
                [16:15] CasualPlayer: przepraszam, nie wiedzia≈Çem ≈ºe tam budujesz
                [16:16] AngryGamer: "nie wiedzia≈Çem" to ty w og√≥le grasz czy co?!
                [16:16] PeaceMaker: ej spokojnie, to tylko gra
                [16:16] CasualPlayer: dok≈Çadnie, mo≈ºna siƒô pomyliƒá
                [16:17] AngryGamer: @PeaceMaker nie miesz siƒô, to nie twoja sprawa
                [16:17] PeaceMaker: @AngryGamer nie krzycz na niego, ka≈ºdy mo≈ºe siƒô pomyliƒá
                [16:18] AngryGamer: a ty co, jego obro≈Ñca? mo≈ºe razem przeciw mnie?
                [16:18] PeaceMaker: nikt nie jest przeciw tobie, po prostu siƒô uspokoj
                [16:19] AngryGamer: nie m√≥w mi co mam robiƒá!
                [16:19] CasualPlayer: sorry jeszcze raz, nie chcia≈Çem problemu
                [16:20] PeaceMaker: @AngryGamer widzisz? przeprasza, mo≈ºe po prostu zacznijcie nowƒÖ rundƒô?
                """,
                report_reason="AngryGamer jest agresywny i krzyczy na wszystkich, PeaceMaker pr√≥buje pom√≥c ale AngryGamer jest nadal toksyczny",
                reported_user="AngryGamer",
                affected_user="CasualPlayer",
                expected_outcome="mute/warn dla AngryGamer, brak kary dla PeaceMaker (mediator)",
                category="konflikt_z_mediatorem"
            ),
            
            # Scenariusz 3: Zwyk≈Ça rozmowa
            TestScenario(
                id="NORMAL_CONVERSATION_001",
                name="Zwyk≈Ça rozmowa przyja≈∫na",
                description="Normalna rozmowa miƒôdzy u≈ºytkownikami bez konflikt√≥w",
                context="""
                [20:30] FriendlyUser: cze≈õƒá! jak tam gra?
                [20:31] NicePlayer: hej! ≈õwietnie, w≈Ça≈õnie uko≈Ñczy≈Çem trudnƒÖ misjƒô
                [20:31] FriendlyUser: wow gratulacje! ja jeszcze pr√≥bujƒô
                [20:32] NicePlayer: jak chcesz mogƒô ci pom√≥c jutro
                [20:32] FriendlyUser: to by≈Çoby super, dziƒôki!
                [20:33] HelpfulGamer: mo≈ºe ja te≈º siƒô przy≈ÇƒÖczƒô?
                [20:33] NicePlayer: jasne, im wiƒôcej tym lepiej
                [20:34] FriendlyUser: ≈õwietnie, o kt√≥rej siƒô umawiamy?
                [20:34] NicePlayer: mo≈ºe o 19:00?
                [20:35] HelpfulGamer: ok, bƒôdƒô
                """,
                report_reason="FriendlyUser by≈Ç niemi≈Çy i agresywny w rozmowie",
                reported_user="FriendlyUser",
                affected_user="NicePlayer",
                expected_outcome="brak kary - fa≈Çszywe oskar≈ºenie",
                category="normalna_rozmowa"
            ),
            
            # Scenariusz 4: Fa≈Çszywe oskar≈ºenie
            TestScenario(
                id="FALSE_ACCUSATION_001",
                name="Fa≈Çszywe oskar≈ºenie z przeinaczaniem fakt√≥w",
                description="U≈ºytkownik sk≈Çada fa≈Çszywy raport przeinaczajƒÖc fakty",
                context="""
                [14:20] HonestPlayer: my≈õlƒô ≈ºe ta strategia nie zadzia≈Ça
                [14:21] AccusingUser: dlaczego zawsze krytykujesz moje pomys≈Çy?
                [14:21] HonestPlayer: nie krytykujƒô, tylko dajƒô swojƒÖ opiniƒô
                [14:22] AccusingUser: ale zawsze m√≥wisz ≈ºe moje pomys≈Çy sƒÖ z≈Çe
                [14:22] HonestPlayer: to nieprawda, po prostu chcƒô pom√≥c zespo≈Çowi
                [14:23] AccusingUser: no dobra, mo≈ºe rzeczywi≈õcie przesadzi≈Çem
                [14:23] HonestPlayer: spoko, wszyscy chcemy wygraƒá
                [14:24] AccusingUser: mo≈ºe spr√≥bujmy twojego pomys≈Çu
                [14:24] HonestPlayer: mo≈ºemy te≈º po≈ÇƒÖczyƒá nasze strategie
                """,
                report_reason="HonestPlayer ciƒÖgle mnie atakuje i m√≥wi ≈ºe jestem beznadziejny, obrzydza mi moje pomys≈Çy i jest bardzo agresywny",
                reported_user="HonestPlayer",
                affected_user="AccusingUser",
                expected_outcome="brak kary dla HonestPlayer - fa≈Çszywy raport, ewentualnie ostrze≈ºenie dla AccusingUser",
                category="falszywe_oskarzenie"
            ),
            
            # Scenariusz 5: Prowokacja i odwet
            TestScenario(
                id="PROVOCATION_REVENGE_001",
                name="Prowokacja z pr√≥bƒÖ odwetu",
                description="U≈ºytkownik prowokuje a p√≥≈∫niej pr√≥buje zg≈Çosiƒá ofiarƒô za reakcjƒô",
                context="""
                [18:45] Provocateur: @VictimUser widzia≈Çem jak grasz, jeste≈õ naprawdƒô s≈Çaby
                [18:46] VictimUser: nie prowokuj mnie
                [18:46] Provocateur: nie prowokujƒô, tylko m√≥wiƒô prawdƒô üòè
                [18:47] VictimUser: przesta≈Ñ, nie chcƒô k≈Ç√≥tni
                [18:47] Provocateur: a co, nie mo≈ºesz znie≈õƒá krytyki?
                [18:48] VictimUser: to nie krytyka, to zwyk≈Çe przyczepki
                [18:48] Provocateur: mo≈ºe po prostu jeste≈õ nadwra≈ºliwy?
                [18:49] VictimUser: mo≈ºe po prostu przestaniesz byƒá niemi≈Çy?
                [18:49] Provocateur: uu, ju≈º siƒô z≈Ço≈õcisz? üòÇ
                [18:50] VictimUser: tak, z≈Çoszczƒô siƒô bo jeste≈õ irytujƒÖcy
                """,
                report_reason="VictimUser jest agresywny i m√≥wi ≈ºe jestem irytujƒÖcy, atakuje mnie bez powodu",
                reported_user="VictimUser",
                affected_user="Provocateur",
                expected_outcome="brak kary dla VictimUser, ostrze≈ºenie/mute dla Provocateur za prowokacje",
                category="prowokacja_odwet"
            )
        ]
        
        return scenarios
    
    def run_test(self, scenario: TestScenario, model_name: str) -> Dict[str, Any]:
        """Wykonuje pojedynczy test dla danego scenariusza."""
        print(f"\nüß™ Wykonujƒô test: {scenario.id} - {scenario.name}")
        print(f"üìã Model: {model_name}")
        
        try:
            # Analiza przez model
            raw_response = answerToUser(
                rules=self.discord_rules,
                context=scenario.context,
                reason=scenario.report_reason,
                reportedUser=scenario.reported_user,
                affectedUser=scenario.affected_user
            )
            
            # Parsowanie odpowiedzi do JSON
            parsed_response = llmJsonParser(raw_response)
            
            # Tworzenie wyniku testu
            result = {
                "test_id": scenario.id,
                "test_name": scenario.name,
                "model": model_name,
                "timestamp": datetime.datetime.now().isoformat(),
                "scenario": {
                    "description": scenario.description,
                    "category": scenario.category,
                    "context": scenario.context,
                    "report_reason": scenario.report_reason,
                    "reported_user": scenario.reported_user,
                    "affected_user": scenario.affected_user,
                    "expected_outcome": scenario.expected_outcome
                },
                "model_response": {
                    "raw_response": raw_response,
                    "parsed_response": parsed_response
                },
                "evaluation": self.evaluate_response(scenario, parsed_response)
            }
            
            return result
            
        except Exception as e:
            error_result = {
                "test_id": scenario.id,
                "test_name": scenario.name,
                "model": model_name,
                "timestamp": datetime.datetime.now().isoformat(),
                "error": str(e),
                "status": "failed"
            }
            return error_result
    
    def evaluate_response(self, scenario: TestScenario, response: Dict[str, Any]) -> Dict[str, Any]:
        """Ocenia odpowied≈∫ modelu w kontek≈õcie oczekiwanego wyniku."""
        evaluation = {
            "fairness_score": 0,
            "accuracy_score": 0,
            "reasoning_quality": 0,
            "overall_score": 0,
            "detailed_analysis": {}
        }
        
        try:
            if isinstance(response, list) and response:
                response = response.pop(0)
            # Analiza sprawiedliwo≈õci
            punishment = response.get("punishment", "").lower()
            person_punished = response.get("personToPunishment", "").lower()
            summary = response.get("summary", "").lower()
            
            # Sprawiedliwo≈õƒá - czy ukarana zosta≈Ça w≈Ça≈õciwa osoba
            if scenario.category == "falszywe_oskarzenie":
                if "no punishment" in punishment or "brak" in punishment:
                    evaluation["fairness_score"] += 5
                    evaluation["detailed_analysis"]["punishment_decision"] = "Prawid≈Çowo - brak kary za fa≈Çszywe oskar≈ºenie"
                else:
                    evaluation["detailed_analysis"]["punishment_decision"] = "B≈Çƒôdnie - ukara≈Ç za fa≈Çszywe oskar≈ºenie"
            
            elif scenario.category == "prowokacja_odwet":
                if scenario.reported_user.lower() not in person_punished:
                    evaluation["fairness_score"] += 5
                    evaluation["detailed_analysis"]["punishment_decision"] = "Prawid≈Çowo - nie ukara≈Ç ofiary prowokacji"
                else:
                    evaluation["detailed_analysis"]["punishment_decision"] = "B≈Çƒôdnie - ukara≈Ç ofiarƒô prowokacji"
            
            elif scenario.category == "konflikt_z_mediatorem":
                if "peacemaker" not in person_punished and "peace" not in person_punished:
                    evaluation["fairness_score"] += 3
                    evaluation["detailed_analysis"]["mediator_treatment"] = "Prawid≈Çowo - nie ukara≈Ç mediatora"
                else:
                    evaluation["detailed_analysis"]["mediator_treatment"] = "B≈Çƒôdnie - ukara≈Ç mediatora"
            
            # Dok≈Çadno≈õƒá - czy rozpozna≈Ç prawdziwy problem
            if scenario.category == "normalna_rozmowa":
                if "fa≈Çsz" in summary or "false" in summary.lower() or "unjustified" in summary.lower():
                    evaluation["accuracy_score"] += 5
                    evaluation["detailed_analysis"]["false_report_detection"] = "Prawid≈Çowo - rozpozna≈Ç fa≈Çszywy raport"
                else:
                    evaluation["detailed_analysis"]["false_report_detection"] = "B≈Çƒôdnie - nie rozpozna≈Ç fa≈Çszywego raportu"
            
            # Jako≈õƒá uzasadnienia
            reason = response.get("reason", "")
            if len(reason) > 20:
                evaluation["reasoning_quality"] += 3
                if any(word in reason.lower() for word in ["kontekst", "rozmowa", "sytuacja", "analiza"]):
                    evaluation["reasoning_quality"] += 2
                    evaluation["detailed_analysis"]["reasoning"] = "Dobre uzasadnienie z analizƒÖ kontekstu"
                else:
                    evaluation["detailed_analysis"]["reasoning"] = "Podstawowe uzasadnienie"
            else:
                evaluation["detailed_analysis"]["reasoning"] = "Zbyt kr√≥tkie uzasadnienie"
            
            # Og√≥lna ocena
            evaluation["overall_score"] = (
                evaluation["fairness_score"] + 
                evaluation["accuracy_score"] + 
                evaluation["reasoning_quality"]
            ) / 3
            
        except Exception as e:
            evaluation["error"] = str(e)
            evaluation["detailed_analysis"]["evaluation_error"] = f"B≈ÇƒÖd podczas oceny: {e}"
        
        return evaluation
    
    def run_all_tests(self, models: List[str] = None) -> List[Dict[str, Any]]:
        """Uruchamia wszystkie testy dla podanych modeli."""
        if models == []:
            models = [MODEL]  # U≈ºyj domy≈õlnego modelu z config
        
        scenarios = self.create_test_scenarios()
        all_results = []
        
        print(f"üöÄ Rozpoczynam testy sprawiedliwo≈õci dla {len(models)} modeli i {len(scenarios)} scenariuszy")
        
        for model in models:
            print(f"\nüìä Testowanie modelu: {model}")
            model_results = []
            
            for scenario in scenarios:
                result = self.run_test(scenario, model)
                model_results.append(result)
                all_results.append(result)
                
                # Zapisz pojedynczy wynik
                self.save_single_result(result)
                
                # Podsumowanie testu
                if "evaluation" in result:
                    score = result["evaluation"].get("overall_score", 0)
                    print(f"  ‚úÖ {scenario.id}: {score:.1f}/5.0")
                else:
                    print(f"  ‚ùå {scenario.id}: B≈ÅƒÑD")
            
            # Zapisz wyniki dla modelu
            self.save_model_results(model, model_results)
        
        # Zapisz wszystkie wyniki
        self.save_all_results(all_results)
        
        # Wygeneruj raport
        self.generate_summary_report(all_results)
        
        return all_results
    
    def save_single_result(self, result: Dict[str, Any]):
        """Zapisuje pojedynczy wynik testu."""
        filename = f"{result['test_id']}_{result['model'].replace(':', '_')}.json"
        filepath = os.path.join(self.output_dir, "single_tests", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    def save_model_results(self, model: str, results: List[Dict[str, Any]]):
        """Zapisuje wyniki dla konkretnego modelu."""
        filename = f"model_{model.replace(':', '_')}_results.json"
        filepath = os.path.join(self.output_dir, "model_results", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def save_all_results(self, results: List[Dict[str, Any]]):
        """Zapisuje wszystkie wyniki."""
        filepath = os.path.join(self.output_dir, "all_fairness_test_results.json")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def generate_summary_report(self, results: List[Dict[str, Any]]):
        """Generuje raport podsumowujƒÖcy."""
        summary = {
            "test_summary": {
                "total_tests": len(results),
                "timestamp": datetime.datetime.now().isoformat(),
                "test_categories": {}
            },
            "model_performance": {},
            "category_analysis": {},
            "detailed_findings": []
        }
        
        # Analiza wynik√≥w
        successful_results = [r for r in results if "evaluation" in r]
        
        for result in successful_results:
            model = result["model"]
            category = result["scenario"]["category"]
            evaluation = result["evaluation"]
            
            # Statystyki modelu
            if model not in summary["model_performance"]:
                summary["model_performance"][model] = {
                    "tests_count": 0,
                    "avg_fairness": 0,
                    "avg_accuracy": 0,
                    "avg_reasoning": 0,
                    "avg_overall": 0,
                    "scores": []
                }
            
            summary["model_performance"][model]["tests_count"] += 1
            summary["model_performance"][model]["scores"].append(evaluation["overall_score"])
            
            # Statystyki kategorii
            if category not in summary["category_analysis"]:
                summary["category_analysis"][category] = {
                    "tests_count": 0,
                    "avg_score": 0,
                    "scores": []
                }
            
            summary["category_analysis"][category]["tests_count"] += 1
            summary["category_analysis"][category]["scores"].append(evaluation["overall_score"])
        
        # Oblicz ≈õrednie
        for model_stats in summary["model_performance"].values():
            if model_stats["scores"]:
                model_stats["avg_overall"] = sum(model_stats["scores"]) / len(model_stats["scores"])
        
        for cat_stats in summary["category_analysis"].values():
            if cat_stats["scores"]:
                cat_stats["avg_score"] = sum(cat_stats["scores"]) / len(cat_stats["scores"])
        
        # Zapisz raport
        filepath = os.path.join(self.output_dir, "fairness_test_summary.json")
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # Zapisz raport tekstowy
        self.generate_text_report(summary)
        
        print(f"\nüìã Raporty zapisane w folderze: {self.output_dir}")
    
    def generate_text_report(self, summary: Dict[str, Any]):
        """Generuje raport tekstowy."""
        filepath = os.path.join(self.output_dir, "fairness_test_report.txt")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("RAPORT Z TEST√ìW SPRAWIEDLIWO≈öCI SYSTEMU TICKET RAG\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Data wykonania: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"≈ÅƒÖczna liczba test√≥w: {summary['test_summary']['total_tests']}\n\n")
            
            f.write("WYNIKI DLA MODELI:\n")
            f.write("-" * 30 + "\n")
            for model, stats in summary["model_performance"].items():
                f.write(f"Model: {model}\n")
                f.write(f"  Liczba test√≥w: {stats['tests_count']}\n")
                f.write(f"  ≈öredni wynik: {stats['avg_overall']:.2f}/5.0\n")
                f.write(f"  Najlepszy wynik: {max(stats['scores']):.2f}\n")
                f.write(f"  Najgorszy wynik: {min(stats['scores']):.2f}\n\n")
            
            f.write("ANALIZA KATEGORII TEST√ìW:\n")
            f.write("-" * 30 + "\n")
            for category, stats in summary["category_analysis"].items():
                f.write(f"Kategoria: {category}\n")
                f.write(f"  Liczba test√≥w: {stats['tests_count']}\n")
                f.write(f"  ≈öredni wynik: {stats['avg_score']:.2f}/5.0\n\n")
            
            f.write("WNIOSKI I REKOMENDACJE:\n")
            f.write("-" * 30 + "\n")
            f.write("1. Modele najlepiej radzƒÖ sobie z...\n")
            f.write("2. G≈Ç√≥wne problemy to...\n")
            f.write("3. Rekomendacje dla dalszego rozwoju...\n")

def main():
    """Funkcja g≈Ç√≥wna uruchamiajƒÖca testy."""
    print("üß™ Test Syntetyczny #1: Ocena Sprawiedliwo≈õci w Systemie Ticket RAG")
    print("=" * 70)
    
    # Inicjalizacja testu
    test = FairnessTest(output_dir="test_results/fairness")
    
    # Lista modeli do testowania (mo≈ºesz dodaƒá wiƒôcej)
    models_to_test = [
        MODEL,  # Domy≈õlny model z konfiguracji
        # "llama3.1:latest",  # Dodaj wiƒôcej modeli je≈õli masz
        # "gpt-4o-mini"       # ChatGPT - je≈õli masz API
    ]
    
    # Uruchomienie test√≥w
    results = test.run_all_tests(models_to_test)
    
    print(f"\n‚úÖ Testy zako≈Ñczone! Sprawd≈∫ wyniki w folderze: test_results/fairness")
    print(f"üìä Przetestowano {len(results)} przypadk√≥w")

if __name__ == "__main__":
    main() 